{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_VBRdfYJ9Rs"
   },
   "source": [
    "# PART 1: Preparations and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_kvulgfdVr4"
   },
   "source": [
    "In this notebook, I will build a CNN-based network that is able to detect whether images of faces have been photoshopped or not. First, let´s load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42JOT2iyezwd"
   },
   "outputs": [],
   "source": [
    "# This needs to be executed beforehand if keras_vggface is not installed yet.\n",
    "pip install git+https://github.com/rcmalli/keras-vggface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPS8QVIdQpdr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras_vggface import utils\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "np.random.seed(123)\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAejBW7ufS1L"
   },
   "source": [
    "The data provided consists of pictures of faces that have, and pictures of faces that have not been photoshopped. A third set of pictures is given, for which we are asked to predict whether these pictures have been photoshopped or not.\n",
    "\n",
    "Let´s start with preprocessing the images by converting and storing them as multidimensional numpy-arrays. As I am going to use different pretrained models later on, the image data needs to be preprocessed in different ways. The reason for this step is the fact that images should be scaled the same way as the training data that had been used by the researchers when training the model. This includes whether images were rescaled to values between zero and one, or minus one and one, or whether they just need to be centered using the mean values of the RGB-channels of the image data used by the researchers. These differently preprcessed images are then stored in different numpy arrays and will be used when the respective pretrained model requires the specific preprocessing of the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5PGrV0wQWCl"
   },
   "outputs": [],
   "source": [
    "l =[]\n",
    "l0 = []\n",
    "l1 = []\n",
    "l2 = []\n",
    "\n",
    "for file in glob.glob('/content/drive/My Drive/Colab Notebooks/data/real/*.jpg'):\n",
    "    x_add = load_img(file, target_size=(224, 224))\n",
    "    x_add = img_to_array(x_add)\n",
    "    x_add0 = preprocess_input(x_add)\n",
    "    x_add1 = utils.preprocess_input(x_add, version=1)\n",
    "    x_add2 = utils.preprocess_input(x_add, version=2)\n",
    "    l.append(x_add)\n",
    "    l0.append(x_add0)\n",
    "    l1.append(x_add1)\n",
    "    l2.append(x_add2)\n",
    "\n",
    "for file in glob.glob('/content/drive/My Drive/Colab Notebooks/data/fake/*.jpg'):\n",
    "    x_add = load_img(file, target_size=(224, 224))\n",
    "    x_add = img_to_array(x_add)\n",
    "    x_add0 = preprocess_input(x_add)\n",
    "    x_add1 = utils.preprocess_input(x_add, version=1)\n",
    "    x_add2 = utils.preprocess_input(x_add, version=2)\n",
    "    l.append(x_add)\n",
    "    l0.append(x_add0)\n",
    "    l1.append(x_add1)\n",
    "    l2.append(x_add2)\n",
    "\n",
    "x = np.stack((l))\n",
    "x0 = np.stack((l0))\n",
    "x1 = np.stack((l1))\n",
    "x2 = np.stack((l2))\n",
    "\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x\", x)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x0\", x0)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x1\", x1)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x2\", x2)\n",
    "\n",
    "l =[]\n",
    "l0 = []\n",
    "l1 = []\n",
    "l2 = []\n",
    "names = []\n",
    "\n",
    "for file in glob.glob('/content/drive/My Drive/Colab Notebooks/data/unknown/*.jpg'):\n",
    "    name = file\n",
    "    names.append(name)\n",
    "    x_add = load_img(file, target_size=(224, 224))\n",
    "    x_add = img_to_array(x_add)\n",
    "    x_add0 = preprocess_input(x_add)\n",
    "    x_add1 = utils.preprocess_input(x_add, version=1)\n",
    "    x_add2 = utils.preprocess_input(x_add, version=2)\n",
    "    l.append(x_add)\n",
    "    l0.append(x_add0)\n",
    "    l1.append(x_add1)\n",
    "    l2.append(x_add2)\n",
    "\n",
    "x_pred = np.stack((l))\n",
    "x_pred0 = np.stack((l0))\n",
    "x_pred1 = np.stack((l1))\n",
    "x_pred2 = np.stack((l2))\n",
    "\n",
    "for num in range(481):\n",
    "  names[num] = names[num].split(\"/\")[-1]\n",
    "for num in range(481):\n",
    "  names[num] = names[num].split(\".\")[0]\n",
    "\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_pred\", x_pred)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_pred0\", x_pred0)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_pred1\", x_pred1)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_pred2\", x_pred2)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/names\", names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1m_T_E0eGw-"
   },
   "source": [
    "To clarify, the mean values used in the preprocessing functions from keras_vggface to substract from the original values are as follows (for the Red, Green, and Blue channel respectively), while no scaling is performed:\n",
    "\n",
    "```\n",
    "x1[..., 0] -= 93.5940\n",
    "x1[..., 1] -= 104.7624\n",
    "x1[..., 2] -= 129.1863\n",
    "\n",
    "x2[..., 0] -= 129.1863\n",
    "x2[..., 1] -= 103.8827\n",
    "x2[..., 2] -= 131.0912\n",
    "```\n",
    "The resulting arrays have a shape of (224,224,3), which corresponds to the number of pictures given, the number of pixels in the horizontal and vertical dimension, and the three colour channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1564506152374,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "dl-i7EPxznj9",
    "outputId": "96e5bdfa-ee72-4963-a88e-41112fed5786"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1354, 224, 224, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1564505542155,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "PRD66s1vxjBx",
    "outputId": "f75992cc-1940-4cb9-afe9-edda951fe781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([188., 173.,  78.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# representation of the first pixel of the first picture (unpreprocessed)\n",
    "x[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 580,
     "status": "ok",
     "timestamp": 1564505562226,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "N5TtKDqAxkwu",
    "outputId": "59058fc5-67a3-4837-db3b-3e1bd4ee0d97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 91.4953, 103.8827, 131.0912], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessed pixels are obtained by substracting the respective mean from the training data used by the researchers\n",
    "x[0,0,0]-x2[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiMwLPkoz44A"
   },
   "outputs": [],
   "source": [
    "#for internal use\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x.npy\" \"x.npy\"\n",
    "x = np.load(\"x.npy\")\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x0.npy\" \"x0.npy\"\n",
    "x0 = np.load(\"x0.npy\")\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x1.npy\" \"x1.npy\"\n",
    "x1 = np.load(\"x1.npy\")\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x2.npy\" \"x2.npy\"\n",
    "x2 = np.load(\"x2.npy\")\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_pred.npy\" \"x_pred.npy\"\n",
    "x_pred = np.load(\"x_pred.npy\")\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_pred0.npy\" \"x_pred0.npy\"\n",
    "x_pred0 = np.load(\"x_pred0.npy\")\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_pred1.npy\" \"x_pred1.npy\"\n",
    "x_pred1 = np.load(\"x_pred1.npy\")\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_pred1.npy\" \"x_pred1.npy\"\n",
    "x_pred1 = np.load(\"x_pred1.npy\")\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_pred2.npy\" \"x_pred2.npy\"\n",
    "x_pred2 = np.load(\"x_pred2.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIbZsC790-uK"
   },
   "source": [
    "Lastly, the target array y is created in a straightforward manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1_GhavMlbrs"
   },
   "outputs": [],
   "source": [
    "y = np.concatenate((np.repeat(0, 891),np.repeat(1, 463)), axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiK84VIkMnLE"
   },
   "source": [
    "As the data is ordered by first listing the non-photoshopped and then listing the photoshopped pictures, it is important to always shuffle the data when creating the training and test set, while additional shuffling between epochs might also be an option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4ywY_7stUg_"
   },
   "source": [
    "# PART 2: Creating and Training Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EiBFypsWIUu"
   },
   "source": [
    "## PART 2.1: Building a Model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVZUd49alOyH"
   },
   "source": [
    "First, let´s start with creating a simple CNN-based model from scratch, consisting of three convolutional layers. Before training, the image data is rescaled to values between zero and one, as usual learning rates might otherwise be too small to optimize the network´s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLSHCZJlyTKS"
   },
   "outputs": [],
   "source": [
    "x = np.divide(x, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IbA0eAcymVP"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, shuffle=True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-LJ3WORwbr_"
   },
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4647,
     "status": "ok",
     "timestamp": 1564326338518,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "eoiU7YUYGl29",
    "outputId": "70ade4f8-165c-435d-c0ce-d8814cc555f7"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "actual = y_test\n",
    "#print('AUC: ', metrics.roc_auc_score(actual, predictions))\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(actual, predictions)\n",
    "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1058,
     "status": "ok",
     "timestamp": 1564326385903,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "V0X_InBRHgxK",
    "outputId": "31b497bc-1212-4b78-90de-ab957d67a7d7"
   },
   "outputs": [],
   "source": [
    "plt.plot(story.history['acc'])\n",
    "plt.plot(story.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyxDZUEbUiHf"
   },
   "source": [
    "## Part 2.2: Implementing Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1peUxRUmsBm"
   },
   "source": [
    "Several networks that already have been trained on image data are publicly available. In total, I choose to implement four of them. Highly relevant for the application of face images should be the pretrained models developed for face recognition by the Visual Geometric Group from the University of Oxford. Here, the library \"keras_vggface\" (installation see Part 0) makes it fairly easy to load and use severeal of their models, i.e. their pretrained models based on the VGG16, the ResNet50, and the SENet50 architecture. The respective papers can be found <a href='http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf'>here</a> and  <a href='https://arxiv.org/pdf/1710.08092.pdf'>here</a>. The ResNet50-based network, for example, has been trained on 3.3 million face images.\n",
    "\n",
    "Furthermore, Keras itself provides several models that were pretrained on the ImageNet dataset. Although these models should be less relevant for this application due to the ImageNet dataset not being restricted to face images, I will implement the VGG16 model trained on the ImageNet dataset as a benchmark.\n",
    "\n",
    "All four pretrained networks consist of two parts: The first part is always a sophisticated structure of convolutional layers, while the second one is a relatively simple multilayer perceptron used to classify the images. As the top networks were trained for a different purpose than detecting whether face images have been photoshopped or not, the second part of the network is rather uninteresting for me. For example, the ResnNet50-based network was trained to classify to which of 9131 persons a picture belongs to. Yet, in the spirit of transfer learning, the first part of the networks should be valuable. Desite not being interested in the classification of the 9131 individuals, one can use the first part of this network as a feature extractor of face pictures in general. After that, these extracted features can be used to train a simple network from scratch for another purpose, in this case the detection of photoshopped faces.\n",
    "\n",
    "To work computationally efficient, I will first extract the features from the respective preprocessed numpy arrays using the different pretrained models and save these features in a separate numpy file. By doing so, I only need to extract the features of the images once and not for every epoch of training the top network. This decreases the training time needed for one epoch of the top network from minutes to a few seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MZKaqaEliHU"
   },
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LG2Wq3hoW3sl"
   },
   "source": [
    "First, lets extract the features using the different pretrained models by feeding the respective preprocessed image data to the network. As the include_top=False argument also excludes the last Flatten layer, I delete the top layers manually and keep the Flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phj01yzVZYRT"
   },
   "outputs": [],
   "source": [
    "featuremodel = VGGFace(model='resnet50')\n",
    "for i in range(0, 1):\n",
    "    featuremodel.layers.pop()\n",
    "x_resnet_faces = featuremodel.predict(x2)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_resnet_faces\", x_resnet_faces)\n",
    "dim_resnet = 8631\n",
    "\n",
    "\n",
    "\n",
    "featuremodel = VGGFace(model='senet50')\n",
    "for i in range(0, 1):\n",
    "    featuremodel.layers.pop()\n",
    "x_senet_faces = featuremodel.predict(x2)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_senet_faces\", x_senet_faces)\n",
    "dim_senet = 8631\n",
    "\n",
    "\n",
    "\n",
    "featuremodel = VGGFace(model='vgg16')\n",
    "for i in range(0, 6):\n",
    "    featuremodel.layers.pop()\n",
    "x_vgg16_faces = featuremodel.predict(x1)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_vgg16_faces\", x_vgg16_faces)\n",
    "dim_vgg16 = 2622\n",
    "\n",
    "\n",
    "\n",
    "featuremodel = VGG16(weights='imagenet')\n",
    "for i in range(0, 3):\n",
    "    featuremodel.layers.pop()\n",
    "    x_vgg16_imagenet = featuremodel.predict(x0)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_vgg16_imagenet\", x_vgg16_imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8W-GLbcIJpq"
   },
   "outputs": [],
   "source": [
    "#for internal use\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_resnet_faces.npy\" \"x_resnet_faces.npy\"\n",
    "x_resnet_faces = np.load(\"x_resnet_faces.npy\")\n",
    "dim_resnet = 8631\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_senet_faces.npy\" \"x_senet_faces.npy\"\n",
    "x_senet_faces = np.load(\"x_senet_faces.npy\")\n",
    "dim_senet = 2622\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_vgg16_faces.npy\" \"x_vgg16_faces.npy\"\n",
    "x_vgg16_faces = np.load(\"x_vgg16_faces.npy\")\n",
    "dim_vgg16 = 2622\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_vgg16_imagenet.npy\" \"x_vgg16_imagenet.npy\"\n",
    "x_vgg16_imagenet = np.load(\"x_vgg16_imagenet.npy\")\n",
    "dim_vgg16 = 2622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_N9Esc-Kl-78"
   },
   "outputs": [],
   "source": [
    "xx = x_resnet_faces\n",
    "input_dim = dim_resnet\n",
    "x_train, x_test, y_train, y_test = train_test_split(xx, y, test_size = 0.2, shuffle=True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZP4rX66uSXd"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, activation='relu', input_dim = input_dim))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30662,
     "status": "ok",
     "timestamp": 1564405457687,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "GqxVJ8DX1ECM",
    "outputId": "4603453b-5231-4800-c6ee-ea4f648c67b5"
   },
   "outputs": [],
   "source": [
    "story = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1564406629556,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "2XCtY1I-5rQw",
    "outputId": "30fcee20-b7e9-4396-8146-ce651a4dca70"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "actual = y_test\n",
    "#print('AUC: ', metrics.roc_auc_score(actual, predictions))\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(actual, predictions)\n",
    "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 977,
     "status": "ok",
     "timestamp": 1564406622305,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "pqNOlODD5fYA",
    "outputId": "48d2ad7a-b71c-4a59-eb02-2022422658fc"
   },
   "outputs": [],
   "source": [
    "plt.title('Training and Testing Accuracy')\n",
    "plt.plot(story.history['acc'])\n",
    "plt.plot(story.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1564406550946,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "osLEuGRfw2DL",
    "outputId": "0f12de41-544a-4e16-afe9-2be0fa861a4e"
   },
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, np.reshape(model.predict(x_test).astype('int'),newshape = (271,)), labels=range(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kxyxpUeUBMr"
   },
   "source": [
    "## PART 2.3: Implementing Keras' ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xjT5mWizj2M"
   },
   "source": [
    "To help a model to generalize better, one can use Keras' ImageDataGenerator to artificially augment the data by applying random transformations, such as zooming or slightly shifting the picture vertically or horizontally. After these transformations, the respective preprocessing function necessary for the respective pretrained model is applied. To not unnecessarily reduce the performance on the test data, only the preprocessing function is applied on pictures from the test set, without performing any random transformations.\n",
    "\n",
    "Again, to be computationally efficient, the features from the newly generated pictures are extracted by the pretrained models and stored before training the top network. In the following, I will use the ResNet50 network from keras_vggface to extract the features from the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1564255736036,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "C9ucgl6h1LLF",
    "outputId": "3fd16257-fa74-45ea-a813-5d6dca15cf50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1083 images belonging to 2 classes.\n",
      "Found 276 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "def preproc(x):\n",
    "  #x = preprocess_input(x)\n",
    "  #x = utils.preprocess_input(x, version=1)\n",
    "  x = utils.preprocess_input(x, version=2)\n",
    "  return x\n",
    "\n",
    "generator_train = ImageDataGenerator(preprocessing_function = preproc,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range = 0.1, \n",
    "                                   zoom_range = 0.1) \n",
    "\n",
    "generator_test = ImageDataGenerator(preprocessing_function = preproc)\n",
    "\n",
    "set_train = generator_train.flow_from_directory('/content/drive/My Drive/Colab Notebooks/data1/train', target_size = (224, 224), batch_size = 1, class_mode= 'binary')\n",
    "\n",
    "set_test = generator_test.flow_from_directory('/content/drive/My Drive/Colab Notebooks/data1/validation', target_size = (224, 224), batch_size = 1, class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rlgan4AJpcrV"
   },
   "outputs": [],
   "source": [
    "model = VGGFace(model='resnet50')\n",
    "for i in range(0, 1):\n",
    "    model.layers.pop()\n",
    "\n",
    "x_resnet_faces_generated_train = model.predict_generator(set_train, 2000)\n",
    "x_resnet_faces_generated_test = model.predict_generator(set_test, 200)\n",
    "\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_resnet_faces_generated_train\", x_resnet_faces_generated_train)\n",
    "np.save(\"/content/drive/My Drive/Colab Notebooks/tempfiles/x_resnet_faces_generated_test\", x_resnet_faces_generated_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUn_QdcW5CbD"
   },
   "outputs": [],
   "source": [
    "#for internal use\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_resnet_faces_generated_train.npy\" \"x_resnet_faces_generated_train.npy\"\n",
    "x_resnet_faces_generated_train = np.load(\"x_resnet_faces_generated_train.npy\")\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/tempfiles/x_resnet_faces_generated_test.npy\" \"x_resnet_faces_generated_test.npy\"\n",
    "x_resnet_faces_generated_test = np.load(\"x_resnet_faces_generated_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9EZR_WNsG7v"
   },
   "outputs": [],
   "source": [
    "train_data = x_resnet_faces_generated_train\n",
    "test_data = x_resnet_faces_generated_test\n",
    "train_labels = np.array([0] * 1000 + [1] * 1000)\n",
    "test_labels = np.array([0] * 100 + [1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpluwF5Ls0pW"
   },
   "outputs": [],
   "source": [
    "input_dim = 8631\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, activation='relu', input_dim = input_dim))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57792,
     "status": "ok",
     "timestamp": 1564327424143,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "Hl2khzQ0LUeW",
    "outputId": "805d2042-df8c-410f-eb90-e5a510f848cf"
   },
   "outputs": [],
   "source": [
    "story = model.fit(train_data, train_labels, batch_size=64, epochs=10, validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1713,
     "status": "ok",
     "timestamp": 1564327433142,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "RIK_s0R_K2ZV",
    "outputId": "6eb10fea-1a82-4b02-e1a2-3550e13c759f"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "actual = y_test\n",
    "#print('AUC: ', metrics.roc_auc_score(actual, predictions))\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(actual, predictions)\n",
    "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1564327436881,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "hio8wh2OK2JN",
    "outputId": "5594a05e-ec74-4879-8a7a-3f82009f3d1b"
   },
   "outputs": [],
   "source": [
    "plt.plot(story.history['acc'])\n",
    "plt.plot(story.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvgk0cB23Ivo"
   },
   "source": [
    "## Part 2.4: Finetuning Top Layers of Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byijRlBq3np6"
   },
   "source": [
    "Another way of implementing pretrained models is to allow the network to fine-tune the last block of convolutional layers, while freezing all the previous blocks. This is a compromise between tuning the entire pretrained model and only training the top network without touching the convolutional part at all. If I randomly initialize the weights of the top network and set the last convolutional block to be trainable, however, the large gradient updates from the top network would heavily influence the gradients for the convolutional part. To avoid this problem, I first train a \"premodel\" as the top network and then load the learned weights from this premodel as the initial weights for the fine-tuning model.\n",
    "\n",
    "In the following, I will use the VGG16 network trained on face images for that. The model is again not able to generalize on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4zX7efs3IRw"
   },
   "outputs": [],
   "source": [
    "#premodel for the initial weights of the featuremodel\n",
    "\n",
    "!cp \"/content/drive/My Drive/Colab Notebooks/preproc10/x1_vgg16faces_features.npy\" \"x1_vgg16faces_features.npy\"\n",
    "x_vgg16_faces = np.load(\"x1_vgg16faces_features.npy\")\n",
    "dim_vgg16 = 2622\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_vgg16_faces, y, test_size = 0.2, shuffle=True, random_state = 0)\n",
    "\n",
    "premodel = Sequential()\n",
    "premodel.add(Dense(4096, activation='relu', input_dim = dim_vgg16))\n",
    "premodel.add(Dropout(0.5))\n",
    "premodel.add(Dense(4096, activation='relu'))\n",
    "premodel.add(Dropout(0.5))\n",
    "premodel.add(Dense(1000, activation='softmax'))\n",
    "premodel.add(Dropout(0.5))\n",
    "premodel.add(Dense(1, activation='softmax'))\n",
    "\n",
    "premodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "premodel.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test), shuffle=True, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7luSZYLh-O0B"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x1, y, test_size = 0.2, shuffle=True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5JqWWEfk3YTP"
   },
   "outputs": [],
   "source": [
    "#featuremodel with initial weights of the top network from the premodel\n",
    "\n",
    "featuremodel = VGGFace(model='vgg16')\n",
    "for i in range(0, 6):\n",
    "    featuremodel.layers.pop()\n",
    "\n",
    "for layer in featuremodel.layers[:14]: #18\n",
    "    layer.trainable = False\n",
    "\n",
    "finemodel = Sequential()\n",
    "finemodel.add(featuremodel)\n",
    "finemodel.add(Dense(4096, activation='relu', input_dim = dim_vgg16))\n",
    "finemodel.add(Dropout(0.5))\n",
    "finemodel.add(Dense(4096, activation='relu'))\n",
    "finemodel.add(Dropout(0.5))\n",
    "finemodel.add(Dense(1000, activation='softmax'))\n",
    "finemodel.add(Dropout(0.5))\n",
    "finemodel.add(Dense(1, activation='softmax'))\n",
    "\n",
    "finemodel.layers[1].set_weights(premodel.layers[0].get_weights())\n",
    "finemodel.layers[3].set_weights(premodel.layers[2].get_weights())\n",
    "finemodel.layers[5].set_weights(premodel.layers[4].get_weights())\n",
    "finemodel.layers[7].set_weights(premodel.layers[6].get_weights())\n",
    "\n",
    "finemodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4168451,
     "status": "ok",
     "timestamp": 1564411021184,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "yuWTiiyZ6gkt",
    "outputId": "5dd5ab83-bf8c-4e82-c6f5-09f47812e657"
   },
   "outputs": [],
   "source": [
    "story = finemodel.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 138777,
     "status": "ok",
     "timestamp": 1564411166292,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "N8grOP4oMWWc",
    "outputId": "13b8a0ba-a229-475f-b51b-8ac0e97f9c72"
   },
   "outputs": [],
   "source": [
    "predictions = finemodel.predict(x_test)\n",
    "actual = y_test\n",
    "#print('AUC: ', metrics.roc_auc_score(actual, predictions))\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(actual, predictions)\n",
    "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 114032,
     "status": "ok",
     "timestamp": 1564411166296,
     "user": {
      "displayName": "Gabriel Blumenstock",
      "photoUrl": "",
      "userId": "15396447649265516687"
     },
     "user_tz": -120
    },
    "id": "-8i3XN3H61Zp",
    "outputId": "e3672609-791c-4421-a62c-a2a782014039"
   },
   "outputs": [],
   "source": [
    "plt.title('Training and Testing Accuracy')\n",
    "plt.plot(story.history['acc'])\n",
    "plt.plot(story.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "p_VBRdfYJ9Rs",
    "DrYjGL_i9kMb",
    "7EiBFypsWIUu",
    "SyxDZUEbUiHf",
    "3MZKaqaEliHU",
    "ABqBS7ADleMk",
    "4kxyxpUeUBMr",
    "Cvgk0cB23Ivo",
    "a0WcyJ0LSTKB",
    "zIo83VT8cy2K",
    "GP6YZXuidyRr",
    "0ZG_NXXdwGK_"
   ],
   "name": "ADAMS_556774.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
